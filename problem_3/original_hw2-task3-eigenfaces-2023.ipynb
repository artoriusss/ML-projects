{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LmMrlcbe2Qwh"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"ZI6wmW222Qwl"},"source":["# Task 3: Eigenfaces & Fischerfaces (3 pts)\n","\n","### <div align=\"right\"> &copy; Markiian Novosad & Rostyslav Hryniv, 2023 </div>\n","\n","## Completed by:   \n","*   First team member\n","*   Second team member\n","\n","\n","In this task your main goal is to discuss dimensionality reduction techniques, such as **PCA** and **LDA** and their performance on classification tasks.\n","\n","A good source to read more on these methods is the book\n","\n","*   Kevin P. Murphy. [Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html), MIT, 2022. Section 9.2.6 (FLDA), 20.1 (PCA and Eigenfaces)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4Hs6_iNP2Qwo"},"source":["## 1. Lib and Data prep (0.5 pts)\n","#### Let's start with importing all neccesary libraries and preparing our data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2u-PrtX2Qwo"},"outputs":[],"source":["from PIL import Image\n","from sklearn.datasets import fetch_lfw_people\n","import numpy as np\n","from sklearn.svm import SVC\n","import sklearn\n","from matplotlib import pyplot as plt\n","import scipy"]},{"cell_type":"markdown","metadata":{"id":"OlsTHFQH2Qwp"},"source":["Seed libs, so we get the same results every time:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUeBEh_f2Qwq"},"outputs":[],"source":["sklearn.random.seed(1)\n","np.random.seed(1)"]},{"cell_type":"markdown","metadata":{"id":"bQcP40aG2Qwr"},"source":["Fetch data and split them into training and test sets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7ZU6qnp2Qwr"},"outputs":[],"source":["lfw_people = fetch_lfw_people(min_faces_per_person=100, resize=0.3)\n","n,h,w = lfw_people.images.shape\n","labels = lfw_people.target\n","\n","class1, class2 =  # choose your preferred classes, make sure that there is almost equal number of samples for each class\n","\n","x= lfw_people.data[(labels == class1)|(labels==class2)]\n","y = labels[(labels == class2)|(labels==class1)]\n","\n","x_train, x_test, y_train, y_test = # split data using sklearn library\n","x_train.shape, x_test.shape"]},{"cell_type":"markdown","metadata":{"id":"kWuTww0P2Qws"},"source":["Visualise Your data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPhU_vn-2Qwt"},"outputs":[],"source":["    # ========= YOUR CODE STARTS HERE ========= #\n","\n","\n","    # ========== YOUR CODE ENDS HERE ========== #"]},{"cell_type":"markdown","source":["## 2. PCA (1.5 pts)\n"],"metadata":{"id":"37l6jdes2-Ms"}},{"cell_type":"markdown","metadata":{"id":"iD4r5Jja2Qwt"},"source":["### **2.1 (0.7 pt) Implementation**\n","Let's start implementing the **PCA** algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-pBomBI2Qwu"},"outputs":[],"source":["class MYPCA:\n","    def __init__(self) -> None:\n","        pass\n","\n","    def _get_num_components(self, lambdas, threshold ) -> int:\n","        \"\"\"\n","        Calculate minimal number of principal components so that variance explainability >= threshold.\n","        \"\"\"\n","         # ========= YOUR CODE STARTS HERE ========= #\n","        ...\n","\n","         # ========== YOUR CODE ENDS HERE ========== #\n","\n","    def center_data(self, data):\n","        \"\"\"\n","        Center our data by subtracting the mean\n","        \"\"\"\n","         # ========= YOUR CODE STARTS HERE ========= #\n","\n","        self._data_mean = ...\n","        return ...\n","         # ========== YOUR CODE ENDS HERE ========== #\n","\n","\n","    def compute_eigen(self, cov_mat):\n","        \"\"\"\n","        Using built-in numpy functionality, compute eigenvectors and eigenvalues of covariance matrix;\n","        \"\"\"\n","        # ========= YOUR CODE STARTS HERE ========= #\n","        eigenvalues, eigenvectors = ...\n","\n","        # ========== YOUR CODE ENDS HERE ========== #\n","\n","        return eigenvalues, eigenvectors\n","\n","\n","    def fit(self, data, exp_threshold=0.8):\n","        \"\"\"\n","        1. Center the data;\n","        2. Compute covariance matrix\n","        3. Compute optimal k (number of components)\n","        4. Compute top-k eigenvalues from covariance matrix\n","        5. Compute components by projecting data with eigenvectors\n","        \"\"\"\n","        # ========= YOUR CODE STARTS HERE ========= #\n","\n","        data_norm = ...\n","        cov_mat = ...\n","        self.eigenvalues, self.eigenvectors = self.compute_eigen(cov_mat=cov_mat)\n","\n","        self._k_componentes = self._get_num_components(self.eigenvalues, exp_threshold)\n","        top_k_vectors = ...\n","        self.components = top_k_vectors.dot(data_norm)\n","        # ========== YOUR CODE ENDS HERE ========== #\n","\n","\n","    def plot_components(self, im_shape):\n","        \"\"\"\n","        For better understanding, visualise the *self.components* variable.\n","        \"\"\"\n","        # ========= YOUR CODE STARTS HERE ========= #\n","\n","        # ========== YOUR CODE ENDS HERE ========== #\n","\n","    def plot_explainability(self):\n","        \"\"\"\n","        Plot dependence of variance explainability on number of components.\n","        \"\"\"\n","        # ========= YOUR CODE STARTS HERE ========= #\n","        ...\n","        # ========== YOUR CODE ENDS HERE ========== #\n","\n","    def transform(self, data):\n","        \"\"\"\n","        1. Center the data;\n","        2. Projection onto the components\n","        \"\"\"\n","        # ========= YOUR CODE STARTS HERE ========= #\n","\n","        data_norm = ...\n","        return data_norm.dot(...)\n","        # ========== YOUR CODE ENDS HERE ========== #\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Yvdn0TI_2Qwv"},"source":["### **2.2 PCA Classifier (0.4 pts)**\n","Having implemented the **MYPCA** class, let's plot how our components look like and our explainability plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0m1uDdW2Qwv"},"outputs":[],"source":["pca = MYPCA()\n","pca.fit(x_train, exp_threshold=0.8)\n","pca.plot_explainability()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UV3tfiok2Qwv"},"outputs":[],"source":["pca.plot_components()"]},{"cell_type":"markdown","metadata":{"id":"qmoja0dt2Qww"},"source":["Now, let's try to classify the data, using sklearn **SVC**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFD4XK4S2Qww"},"outputs":[],"source":["\n","classifier = SVC().fit(pca.transform(...), ...)\n","print(classifier.score(pca.transform(...), ...))"]},{"cell_type":"markdown","metadata":{"id":"8nHXJxMZ2Qww"},"source":["### **2.3. Discussion of the results (0.4 pts):**\n","1. What is the concept of variance explainability and why is it so important?\n","1. What is the optimal number of components to reach good enough classification performance and why?"]},{"cell_type":"markdown","metadata":{"id":"dN72SKVf2Qww"},"source":["---\n","#### **Your answer here:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"bLJvru6U2Qwx"},"source":["## 3. Fischerface classification and LDA (1 pt)\n","Now, having implemented the **PCA** algorithm, let's implement the **Linear Discriminant Analysis** algorithm.\n","\n","By this [link](https://towardsdatascience.com/fishers-linear-discriminant-intuitively-explained-52a1ba79e1bb) you can read more about this algorithm.\n","\n","In this task we will see how we can improve our classification using **LDA**."]},{"cell_type":"markdown","source":["### **3.1. (0.6 pt) Implementation**\n","Here you only need to compute the $\\mathbf{\\it{S_B}}$ and $\\mathbf{\\it{S_W}}$ covariance matrices, discussed in the source above, and select appropriate eigenvectors"],"metadata":{"id":"FLeEu4aJ0sjg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4nTLBNQ2Qwx"},"outputs":[],"source":["class MYLDA:\n","\n","    def fit(self, data, data_labels, k_components):\n","        n_feats = data.shape[-1]\n","        SW = np.zeros((n_feats, n_feats))\n","        SB = np.zeros((n_feats, n_feats))\n","        mu = data.mean(0)\n","\n","        # ========= YOUR CODE STARTS HERE ========= #\n","\n","        for channel in np.unique(data_labels):\n","            ...\n","        # ========== YOUR CODE ENDS HERE ========== #\n","\n","        evals, evecs = scipy.linalg.eigh(SB, SW)\n","        self.proj = ... # sort eigenvectors by eigenvalues and selet top k eigenvectors\n","\n","    def transform(self, data):\n","        return data @ self.proj\n"]},{"cell_type":"markdown","metadata":{"id":"Sl5PiR9G2Qwx"},"source":["Here we will implement the class for the Fischerface algorithm, which basically uses an **LDA** on top of **PCA**, such that we will perform maximum dimensionality reduction for classification purposes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Zzdo1Fv2Qwx"},"outputs":[],"source":["class FischerFaces:\n","    def __init__(self) -> None:\n","        \"\"\"\n","        Initialize pca and lda\n","        \"\"\"\n","        self.pca = ...\n","        self.lda = ...\n","\n","    def fit(self, X,Y, threshold, lda_components):\n","        \"\"\"\n","        Fit the fischerface algo:\n","        1. Fit PCA\n","        2. Reduce data dimensions with PCA\n","        3. Fit LDA\n","        \"\"\"\n","        self.pca.fit(...)\n","        pca_trans = ...\n","        self.lda.fit( pca_trans,  Y , lda_components )\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Transform the data by using fitted PCA and LDA\n","        \"\"\"\n","        return ..."]},{"cell_type":"markdown","metadata":{"id":"Fxhi_2Lp2Qwy"},"source":["Now, let's try out our new classifier based on Fischerface algorithm.\n","\n","*Hint*: select the number of LDA components equal to the number of classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLHmbeYF2Qwy","outputId":"8200db53-091a-4fbd-fe89-7f8a86e6bb4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7014925373134329\n"]}],"source":["f = FischerFaces()\n","f.compute(x_train, y_train, ..., ...)\n","\n","classifier = SVC().fit(f.transform(x_train), y_train)\n","print(...)"]},{"cell_type":"markdown","metadata":{"id":"yVqC5sng2Qwy"},"source":["### **2.2 Discussion (0.4 pts)**\n","Evaluate the results of Fishcerface classifier (LDA). Discuss whether it performs better than the classic Eigenface algorithm (PCA), and if so, why"]},{"cell_type":"markdown","metadata":{"id":"7lsGW_1B2Qwy"},"source":["---\n","#### **Your explanations come here**\n","---"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}