{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe44083c-b2d1-4370-a08c-53b19b8ec944",
   "metadata": {
    "id": "fe44083c-b2d1-4370-a08c-53b19b8ec944",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use torch and sklearn only on steps of fitting data\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import linprog\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EQznMLwjRzhi",
   "metadata": {
    "id": "EQznMLwjRzhi",
    "tags": []
   },
   "source": [
    "# Problem 2: Separating hyperplanes and the Perceptron Learning Algorithm (3pts)\n",
    "### <div align=\"right\"> &copy; Yurii Yeliseev & Rostyslav Hryniv, 2022 </div>\n",
    "\n",
    "## Completed by:   \n",
    "*   First team member\n",
    "*   Second team member\n",
    "\n",
    "\n",
    "#### The aim of this task is to discuss a simple binary classification method for linearly separated classes. The Perceptron Learning Algorithm finds a ***separating hyperplane*** in finitely many steps and is based on a clear geometric update method. We will derive the upper bound on the number of iterations in PLA and implement it for digit classification for the MNIST database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BPiRUcHnhqxX",
   "metadata": {
    "id": "BPiRUcHnhqxX",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Separating hyperplanes and classification (0.9 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F8uu18FS0NVJ",
   "metadata": {
    "id": "F8uu18FS0NVJ"
   },
   "source": [
    "### 1.1. Binary classification.    \n",
    "A typical task of binary classification reads as follows. We are given the set of labelled (training) data $(\\mathbf{x}_k, y_k), k=1,2,\\dots, N$, where $\\mathbf{x}_k \\in \\mathbb{R}^d$ gives a data point and the label $y_k = \\pm1$ encodes the class (e.g. $y_k=1$ is the <font color='red'>''red''</font> class and $y_k=-1$ is the <font color='blue'>''blue''</font> one). The task is to find a classfier $f \\,:\\, \\mathbb{R}^d \\to \\pm1$ that would correctly recognize the classes, i.e. satisfy $y_k f(\\mathbf{x}_k) >0$ for all (or most) $k=1,2,\\dots,N$. This function can then be used to guess the class of new (unseen) data $\\mathbf{x}\\in\\mathbb{R}^n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HqpK65XMSYul",
   "metadata": {
    "id": "HqpK65XMSYul"
   },
   "source": [
    "\n",
    "### 1.2. Separating hyperplane  \n",
    "The simplest case is when the red and blue classes are *linearly separable*, i.e., when there is a hyperplane $H: \\mathbf{w} \\cdot \\mathbf{x} + w_0 = 0$ separating the red and blue datapoints. Then  $f(\\mathbf{x}) = \\mathbf{w}\\cdot \\mathbf{x} + w_0$ is an affine classifier, so that $f(\\mathbf{x}_k)>0$ for red points and $f(\\mathbf{x}_k)<0$ for blue ones. Augmenting $\\mathbf{x}$ to $\\widehat{\\mathbf{x}} := (1, \\mathbf{x})$ and $\\widehat{\\mathbf{w}} = (w_0,\\mathbf{w})$, we recognize that $f(\\mathbf{x})= \\widehat{\\mathbf{x}}\\cdot \\widehat{\\mathbf{w}}$. Therefore, the angles between $\\widehat{\\mathbf{x}}$ and $\\widehat{\\mathbf{w}}$ are acute for red datapoints and obtuse for the blue ones. The task is therefore to find the *normal vector* $\\widehat{\\mathbf{w}}$ with this properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75612201-411f-48d9-8f36-629d8e8c5811",
   "metadata": {
    "id": "75612201-411f-48d9-8f36-629d8e8c5811",
    "tags": []
   },
   "source": [
    "### 1.3. The idea behind the Perceptron learning algorithm (PLA)\n",
    "\n",
    "To simplify the notations, in what follows we will omit the \"hats\" above the $(d+1)$-dimensional vectors $\\widehat{\\mathbf{x}}$ and $\\widehat{\\mathbf{w}}$.\n",
    "\n",
    "PLA is an iterative algorithm that updates the direction vector ${\\mathbf{w}}$ towards a misclassified example, one at a time.\n",
    "\n",
    "Let's recall that correctly classified vectors $\\mathbf{x}_j$ must satisfy the inequality\n",
    "$$\n",
    "  y_j ({\\mathbf{w}}\\cdot {\\mathbf{x}}_j) > 0.\n",
    "$$\n",
    "If a red $\\mathbf{x}_j$ is misclassified, then the angle between ${\\mathbf{w}}$ and ${\\mathbf{x}}_j$ is obtuse. The idea is that we should decrease the angle between them by updating ${\\mathbf{w}}$ to ${\\mathbf{w}} + {\\mathbf{x}}_j$ (see Figure 1). Likewise, if a blue $\\mathbf{x}_j$ is misclassified, then the angle between ${\\mathbf{w}}$ and ${\\mathbf{x}}_j$ is acute, and we increase it be replacing ${\\mathbf{w}}$ with ${\\mathbf{w}} - {\\mathbf{x}}_j$. In both cases, the update is $${\\mathbf{w}} \\mapsto {\\mathbf{w}} + y_j {\\mathbf{x}}_j$$\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title></title>\n",
    "</head>\n",
    "<body>\n",
    "    <img src=\"https://drive.google.com/uc?export=view&id=12rduejeedS8NxrxXkSBJkkcDH3lB0k-R\">\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "### 1.4. **PLA**\n",
    "\n",
    "The above considerations suggest the following **PLA**:\n",
    "1.   Start with ${\\mathbf{w}}_0=\\mathbf{0}$ and classify the points\n",
    "2.   Take an arbitrary misclassified point\n",
    "3.   Update the ${\\mathbf{w}}$\n",
    "4.   Update the classification\n",
    "5.   Repeat 2-4 until there are misclassified points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oOn-vs9ye4B2",
   "metadata": {
    "id": "oOn-vs9ye4B2"
   },
   "source": [
    "### 1.5. **PLA**: proof of convergence (0.9 pts)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **1.5.1 (0.3 pts)** Analyze the PLA update step  \n",
    "Prove that by updating ${\\mathbf{w}}$, we are decreasing or increasing (as required) the angle between ${\\mathbf{w}}$ and ${\\mathbf{x}}_j$.\n",
    "\n",
    "---\n",
    "\n",
    "First, let's consider a misclassified datapoint ${\\mathbf{x}}_j$, where the product $y_j ({\\mathbf{w}}\\cdot {\\mathbf{x}}_j)$ is negative. This situation occurs either when $y_j = 1$ and the angle between ${\\mathbf{w}}$ and ${\\mathbf{x}}_j$ is obtuse (greater than 90 degrees), or when $y_j = -1$ and the angle is acute (less than 90 degrees).\n",
    "\n",
    "In the perceptron algorithm, the weight vector ${\\mathbf{w}}$ is updated in the direction of $y_j {\\mathbf{x}}_j$:\n",
    "\n",
    "$${\\mathbf{w}}' = {\\mathbf{w}} + y_j {\\mathbf{x}}_j.$$\n",
    "\n",
    "If we consider the dot product of ${\\mathbf{w}}'$ and ${\\mathbf{x}}_j$, we have:\n",
    "\n",
    "$${\\mathbf{w}}'\\cdot {\\mathbf{x}}_j = ({\\mathbf{w}} + y_j {\\mathbf{x}}_j)\\cdot {\\mathbf{x}}_j = {\\mathbf{w}}\\cdot {\\mathbf{x}}_j + y_j {\\mathbf{x}}_j\\cdot {\\mathbf{x}}_j.$$\n",
    "\n",
    "We note that ${\\mathbf{x}}_j\\cdot {\\mathbf{x}}_j$ is always greater than or equal to zero, as it's equivalent to the square of the length of the vector ${\\mathbf{x}}_j$.\n",
    "\n",
    "When $y_j = 1$, the dot product ${\\mathbf{w}}'\\cdot {\\mathbf{x}}_j$ is larger than ${\\mathbf{w}}\\cdot {\\mathbf{x}}_j$ and when $y_j = -1$, the dot product ${\\mathbf{w}}'\\cdot {\\mathbf{x}}_j$ is smaller than ${\\mathbf{w}}\\cdot {\\mathbf{x}}_j$.\n",
    "\n",
    "We recall that the dot product of two vectors can be expressed in terms of the cosine of the angle between them as ${\\mathbf{u}}\\cdot{\\mathbf{v}} = ||\\mathbf{u}||\\hspace{1mm}||\\mathbf{v}|| \\cos(\\theta)$.\n",
    "\n",
    "Hence, if $y_j = 1$, a larger dot product ${\\mathbf{w}}'\\cdot {\\mathbf{x}}_j$ implies that the cosine of the angle after the update is larger, which in turn means that the angle itself becomes smaller, bringing ${\\mathbf{w}}$ closer in alignment to ${\\mathbf{x}}_j$. If $y_j = -1$, a smaller dot product ${\\mathbf{w}}'\\cdot {\\mathbf{x}}_j$ implies the cosine of the angle after the update is smaller (more negative), which increases the angle between ${\\mathbf{w}}$ and ${\\mathbf{x}}_j$, moving ${\\mathbf{w}}$ further away from ${\\mathbf{x}}_j$, as needed to correct the misclassification.\n",
    "\n",
    "Consequently, we can conclude that the update step of the PLA indeed moves the weight vector ${\\mathbf{w}}$ as needed to correct the misclassification.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5680473-6119-4831-8990-ff6b3ac69e83",
   "metadata": {
    "id": "e5680473-6119-4831-8990-ff6b3ac69e83"
   },
   "source": [
    "#### **Assumptions and notations**\n",
    "\n",
    "***Assumption on linear separability*** There exists an ${\\mathbf{w}^{\\star}} \\in \\mathbb{R}^{d+1}$ of unit length and $\\gamma > 0$ such that $$y_k\\, {\\mathbf{x}}_k\\cdot {\\mathbf{w}}^{\\star} \\ge \\gamma, \\qquad k=1,2,\\dots, n.$$ The value $\\gamma$ determines the width of the *separating slab* free of any datapoints. The larger $\\gamma$, the wider the slab and the more robust the classifier is to noise in data.  \n",
    "\n",
    "We also denote by $R$ the maximum norm of $\\mathbf{x}_k$\n",
    "\n",
    "***Theorem on PLA convergence.*** The PLA makes at most $\\dfrac{R^2}{\\gamma^2}$ updates, after which it returns a separating hyperplane.\n",
    "\n",
    "***Proof.*** Should the algorthm terminate, then the resulting ${\\mathbf{w}}$ determines a separating hyperplane. Thus it suffices to show that the algorithm terminates after at most $\\frac{R^2}{\\gamma^2}$ updates. The approach is to get upper and lower bounds on the norm of the $k^{\\mathrm{th}}$ update ${\\mathbf{w}}_k$ of the weighting vector ${\\mathbf{w}}$, starting with ${\\mathbf{w}}_0 = \\mathbf{0}$.\n",
    "\n",
    "Assume that $k\\ge 1$ and ${\\mathbf{x}}_j$ is a misclasssified point on iteration $k$; then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w}_{k+1} \\cdot {\\mathbf{w}}^{\\star} &=\\left({\\mathbf{w}}_k + y_j \\mathbf{x}_j\\right) \\cdot {\\mathbf{w}}^{\\star} \\\\\n",
    "&={\\mathbf{w}}_k \\cdot {\\mathbf{w}}^{\\star}+y_j\\left({\\mathbf{x}}_j \\cdot {\\mathbf{w}}^{\\star}\\right) \\\\\n",
    "&>{\\mathbf{w}}_k \\cdot {\\mathbf{w}}^{\\star} + \\gamma\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.5.2. (0.3 pts)** Explain by induction that ${\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star}> k \\gamma$.\n",
    "\n",
    "---\n",
    "\n",
    "We will prove by mathematical induction that ${\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star} \\ge k \\gamma$ for all $k \\ge 0$.\n",
    "\n",
    "Base Case:\n",
    "\n",
    "When $k=0$, we have ${\\mathbf{w}}_{0} \\cdot {\\mathbf{w}}^{\\star} = 0$, which is indeed $\\ge 0 \\cdot \\gamma$, thus satisfying our base case.\n",
    "\n",
    "Inductive Hypothesis:\n",
    "\n",
    "Assume that ${\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star} \\ge k \\gamma$ holds for some $k \\ge 0$.\n",
    "\n",
    "Inductive Step:\n",
    "\n",
    "Now we need to show that ${\\mathbf{w}}_{k+1} \\cdot {\\mathbf{w}}^{\\star} \\ge (k+1) \\gamma$.\n",
    "\n",
    "We know from our update rule that\n",
    "\n",
    "$${\\mathbf{w}}_{k+1} \\cdot {\\mathbf{w}}^{\\star} = {\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star} + y_j ({\\mathbf{x}}_j \\cdot {\\mathbf{w}}^{\\star}).$$\n",
    "\n",
    "Given the margin condition $y_j ({\\mathbf{x}}_j \\cdot {\\mathbf{w}}^{\\star}) \\ge \\gamma$ for all $j$, we can write\n",
    "\n",
    "$${\\mathbf{w}}_{k+1} \\cdot {\\mathbf{w}}^{\\star} \\ge {\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star} + \\gamma.$$\n",
    "\n",
    "By substituting our induction hypothesis ${\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star} \\ge k \\gamma$ into this inequality, we get\n",
    "\n",
    "$${\\mathbf{w}}_{k+1} \\cdot {\\mathbf{w}}^{\\star} \\ge k \\gamma + \\gamma = (k+1) \\gamma.$$\n",
    "\n",
    "Hence, the inequality ${\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star} \\ge k \\gamma$ holds for all $k \\ge 0$.\n",
    "\n",
    "---\n",
    "\n",
    "This proof shows that with every update of the weight vector, the dot product of ${\\mathbf{w}}_{k}$ and the optimal weight vector ${\\mathbf{w}}^{\\star}$ increases by at least $\\gamma$, ensuring that ${\\mathbf{w}}_{k}$ progressively aligns with ${\\mathbf{w}}^{\\star}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d5fb7-6bc7-444e-a8cc-b57965b57c01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "BfKkTkx2YTrb",
   "metadata": {
    "id": "BfKkTkx2YTrb"
   },
   "source": [
    "As a result, we see that\n",
    "$$\\|\\mathbf{w}_k\\| \\ge {\\mathbf{w}}_{k} \\cdot {\\mathbf{w}}^{\\star}> k \\gamma\\tag{1}$$\n",
    "\n",
    "To obtain the upper bound, we argue that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\|\\mathbf{w}_{k+1}\\right\\|^2 &=\\left\\|\\mathbf{w}_k+y_j \\mathbf{x}_j\\right\\|^2 \\\\\n",
    "&=\\left\\|\\mathbf{w}_k\\right\\|^2+\\left\\|y_j \\mathbf{x}_j\\right\\|^2+2\\left(\\mathbf{w}_k \\cdot \\mathbf{x}_j\\right) y_j \\\\\n",
    "&=\\left\\|\\mathbf{w}_k\\right\\|^2+\\left\\|\\mathbf{x}_j\\right\\|^2+2\\left(\\mathbf{w}_k \\cdot \\mathbf{x}_j\\right) y_j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.5.3. (0.3 pts)** Derive the lower bound\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\|\\mathbf{w}_{k+1}\\right\\|^2\n",
    "&\\le\\left\\|\\mathbf{w}_k\\right\\|^2+\\left\\|\\mathbf{x}_j\\right\\|^2 \\\\\n",
    "&\\le\\left\\|\\mathbf{w}_k\\right\\|^2+R^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "and use induction to conclude that\n",
    "$$\n",
    "\\left\\|\\mathbf{w}_{k}\\right\\|^2 \\le k\\, R^2 \\tag{2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Let's derive the lower bound for the squared norm of $\\mathbf{w}_{k+1}$:\n",
    "\n",
    "Looking at the equation for $\\left\\|\\mathbf{w}_{k+1}\\right\\|^2$, we know that the term $2(\\mathbf{w}_k \\cdot \\mathbf{x}_j) y_j$ is negative because $\\mathbf{x}_j$ is a misclassified point (so $\\mathbf{w}_k \\cdot \\mathbf{x}_j y_j < 0$). Because we're adding a negative number, it will decrease the value of $\\left\\|\\mathbf{w}_{k+1}\\right\\|^2$, so:\n",
    "\n",
    "$$\\left\\|\\mathbf{w}_{k+1}\\right\\|^2 \\le \\left\\|\\mathbf{w}_k\\right\\|^2+\\left\\|\\mathbf{x}_j\\right\\|^2.$$\n",
    "\n",
    "Now, the magnitude of $\\mathbf{x}_j$ is less than or equal to the maximum distance $R$ from the origin, so $\\left\\|\\mathbf{x}_j\\right\\|^2 \\le R^2$. Therefore:\n",
    "\n",
    "$$\\left\\|\\mathbf{w}_{k+1}\\right\\|^2 \\le \\left\\|\\mathbf{w}_k\\right\\|^2+R^2.$$\n",
    "\n",
    "To show that $\\left\\|\\mathbf{w}_{k}\\right\\|^2 \\le k\\, R^2$ by induction, we start with the base case:\n",
    "\n",
    "For $k=0$, $\\left\\|\\mathbf{w}_{0}\\right\\|^2 = 0 \\le 0 = 0 \\cdot R^2$, which is true.\n",
    "\n",
    "Assume that for some $k \\ge 0$, we have $\\left\\|\\mathbf{w}_{k}\\right\\|^2 \\le k\\, R^2$.\n",
    "\n",
    "Now we need to show that $\\left\\|\\mathbf{w}_{k+1}\\right\\|^2 \\le (k+1)\\, R^2$:\n",
    "\n",
    "We know that $\\left\\|\\mathbf{w}_{k+1}\\right\\|^2 \\le \\left\\|\\mathbf{w}_k\\right\\|^2+R^2$ and by the inductive hypothesis we can replace $\\left\\|\\mathbf{w}_k\\right\\|^2$ with $k\\, R^2$:\n",
    "\n",
    "$$\\left\\|\\mathbf{w}_{k+1}\\right\\|^2 \\le k\\, R^2+R^2 = (k+1)\\, R^2.$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\left\\|\\mathbf{w}_{k}\\right\\|^2 \\le k\\, R^2 \\tag{2}\n",
    "$$\n",
    "\n",
    "for all $k \\ge 0$.\n",
    "\n",
    "---\n",
    "\n",
    "Together, (1) and (2) yield\n",
    "$$\n",
    "k^2 \\gamma^2<\\left\\|\\mathbf{w}_{k}\\right\\|^2 \\le k R^2,\n",
    "$$\n",
    "which implies the bound $k<\\frac{R^2}{\\gamma^2}$ and finishes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4vq40CCjb4tp",
   "metadata": {
    "id": "4vq40CCjb4tp"
   },
   "source": [
    "## 2. PLA implementation on MNIST dataset (1.8 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69405653-4bb7-4a5a-a6ab-6d06f81e2452",
   "metadata": {
    "id": "69405653-4bb7-4a5a-a6ab-6d06f81e2452"
   },
   "source": [
    "### 2.1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25024833-b7ae-48a4-b2d2-254aedb9e1ff",
   "metadata": {
    "id": "25024833-b7ae-48a4-b2d2-254aedb9e1ff"
   },
   "source": [
    "`train_data` is torch dataset object where images and targets lie inside `train_data.data` and `train_data.targets` respectively. To convert to numpy array you can use `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689dbfa9-1ed2-4baa-b0f0-8c094fa9a972",
   "metadata": {
    "id": "689dbfa9-1ed2-4baa-b0f0-8c094fa9a972",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root='data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a6dd4-0ffe-4f59-bfad-b33824243772",
   "metadata": {
    "id": "ab0a6dd4-0ffe-4f59-bfad-b33824243772"
   },
   "source": [
    "### 2.2 Take 2 digits samples **(0.3 pts)**\n",
    "\n",
    "First of all you need to take only two digits samples from the dataset and convert the targets properly for the PLA. Choose the two digits based on the sum of your birthdays (e.g. 2 and 4 if it is 24; take 4 and 5 if it is 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf9e98a-daff-4c94-9019-ace288e89bc9",
   "metadata": {
    "id": "8cf9e98a-daff-4c94-9019-ace288e89bc9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_data(train_data, digit_1, digit_2):\n",
    "    \"\"\"\n",
    "    Take only digit_1 and digit_2 from the dataset and transform labels\n",
    "    Args:\n",
    "        train_data: torchvision.datasets.mnist.MNIST\n",
    "        digit_1: int (from 0 to 9)\n",
    "        digit_2: int (from 0 to 9)\n",
    "\n",
    "    Returns:\n",
    "        train_data: torchvision.datasets.mnist.MNIST or np.array\n",
    "    \"\"\"\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    digits_cls = torch.tensor([digit_1, digit_2])\n",
    "    indices = torch.isin(train_data.targets, digits_cls)\n",
    "    train_data.data, train_data.targets = train_data.data[indices], train_data.targets[indices]\n",
    "    # label digit 2 as -1 and 3 as 1\n",
    "    train_data.targets = torch.where((train_data.targets == digit_1), -1, torch.where((train_data.targets == digit_2), 1, train_data.targets))\n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee2eaa6-498f-4174-875a-501548d56113",
   "metadata": {
    "id": "dee2eaa6-498f-4174-875a-501548d56113",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = filter_data(train_data, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c7795f-5ac7-4898-ad18-d5a89b908a35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  1,  1,  ..., -1, -1,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IFga5wKx5qLn",
   "metadata": {
    "id": "IFga5wKx5qLn"
   },
   "source": [
    "### 2.3 Take a smaller subset and divide it into train and test sets **(0.3 pts)**\n",
    "\n",
    "\n",
    "Since the dataset is big, you need to use only part of it in this task (take\n",
    "~20-30% of the whole dataset for further processing).\n",
    "\n",
    "1. Properly subdivide dataset\n",
    "2. Calculate number samples in each class for test and train\n",
    "\n",
    "***Note***: you need to have same distributions inside train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13cxSAQd5qLo",
   "metadata": {
    "id": "13cxSAQd5qLo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(train_data):\n",
    "    \"\"\"\n",
    "    Split dataset into train and test parts.\n",
    "\n",
    "    !Hint: You can use train_test_split from sklearn for that\n",
    "\n",
    "    Args:\n",
    "        train_data: torchvision.datasets.mnist.MNIST or np.array\n",
    "\n",
    "    Returns:\n",
    "        X_train: Array of shape (N, 28, 28), images from the train set\n",
    "        y_train: Array of shape (N), labels from the train set\n",
    "\n",
    "        X_test: Array of shape (N, 28, 28), images from the test set\n",
    "        y_test: Array of shape (N), labels from the test set\n",
    "    \"\"\"\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data.data, train_data.targets, train_size=0.20, test_size=0.05, stratify=train_data.targets)\n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4pT-osLT5qLo",
   "metadata": {
    "id": "4pT-osLT5qLo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "795c31dc-f064-4c8a-ac99-795a389d14ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train: 2417 \n",
      "Classes number in train: 1226, 1191 \n",
      "Number of samples in test: 605 \n",
      "Classes number in test: 307, 298\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in train: {len(X_train)} \\n\\\n",
    "Classes number in train: {torch.sum(y_train == 1)}, {torch.sum(y_train == -1)} \\n\\\n",
    "Number of samples in test: {len(X_test)} \\n\\\n",
    "Classes number in test: {torch.sum(y_test == 1)}, {torch.sum(y_test == -1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KV4wznVX5qLo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KV4wznVX5qLo",
    "outputId": "2ec93ca1-af38-414f-fc25-349053e2c1b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train: 2032 \n",
      "Classes number in train: 1078, 954 \n",
      "Number of samples in test: 508 \n",
      "Classes number in test: 270, 238\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in train: {len(X_train)} \\n\\\n",
    "Classes number in train: {torch.sum(y_train == 1)}, {torch.sum(y_train == -1)} \\n\\\n",
    "Number of samples in test: {len(X_test)} \\n\\\n",
    "Classes number in test: {torch.sum(y_test == 1)}, {torch.sum(y_test == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swE73GJ76GxU",
   "metadata": {
    "id": "swE73GJ76GxU"
   },
   "source": [
    "### 2.4 Visualize samples for the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fbde345-f57b-47ca-b04c-4fc1fd1f32c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "for idx in np.arange(10):\n",
    "    ax = fig.add_subplot(2, 5, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(X_train[idx].numpy()), cmap='gray')\n",
    "    ax.set_title(str(y_train[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X2el3UpD6cYO",
   "metadata": {
    "id": "X2el3UpD6cYO"
   },
   "source": [
    "### 2.5 Preprocess the samples and initialize $\\mathbf{w}$ **(0.4 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531c05a-28a8-4309-98d2-b278e43be21f",
   "metadata": {
    "id": "8531c05a-28a8-4309-98d2-b278e43be21f"
   },
   "source": [
    "The original algorithm starts from zero parameter vector, but actually we can use just randomly initialized vector; it will make it faster to converge\n",
    "\n",
    "**Instructions**: Complete the missing lines of code and calculate the performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a71993e-91fe-45af-bb58-040bbfd0cc40",
   "metadata": {
    "id": "2a71993e-91fe-45af-bb58-040bbfd0cc40",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_data(X_train):\n",
    "    \"\"\"\n",
    "    Flatten, normalize and extra column for bias\n",
    "    Args:\n",
    "        X_train: np.array of shape (N, 28, 28)\n",
    "\n",
    "    Returns:\n",
    "        X: preprocessed data\n",
    "    \"\"\"\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    N = len(X_train)\n",
    "    train_flattened = X_train.reshape(N, -1)\n",
    "    X = np.apply_along_axis(lambda x: x / 255, 1, train_flattened)\n",
    "    X = np.append(X, np.ones((X.shape[0], 1)), 1)\n",
    "    \n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return X\n",
    "\n",
    "def initialize_weight_vector(size):\n",
    "    \"\"\"\n",
    "    Create random parameter vector\n",
    "    Args:\n",
    "        size: Number of elements\n",
    "\n",
    "    Returns:\n",
    "        W: np.array of shape (size)\n",
    "    \"\"\"\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    return np.random.uniform(-1, 1, size)\n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "\n",
    "def misclassified(X, y, W):\n",
    "    \"\"\"\n",
    "    Calculate indices of missclasified points\n",
    "    Args:\n",
    "        X: np.array, training images\n",
    "        y: np.array, training labels\n",
    "        w: np.array, parameter vector\n",
    "\n",
    "    Returns:\n",
    "        M: np.array of shape (m) - indices of missclasified points, where m is a number of missclasified points\n",
    "    \"\"\"\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    w_dot_X = np.dot(X, W)\n",
    "    sign_w_dot_X = np.sign(w_dot_X)\n",
    "    misclassified_mask = y != sign_w_dot_X\n",
    "    misclass_indices = np.where(misclassified_mask)[0]\n",
    "    \n",
    "    return misclass_indices\n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "\n",
    "\n",
    "X_train_flat_aug = prep_data(X_train)\n",
    "W = initialize_weight_vector(X_train_flat_aug.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c942c8e-1a2b-47f0-af9a-8ac09905c8d4",
   "metadata": {
    "id": "4c942c8e-1a2b-47f0-af9a-8ac09905c8d4"
   },
   "source": [
    "### 2.6 Training loop **(0.5 pts)**\n",
    "Here you need to complete the training loop of the PLA algorithm. Observe that recalculation the misclassified set (Step 3 of the PLA algorithm) is the most costly (as we need to iterate through the whole train set). To speed up the algorithm convergence, we will do the following:\n",
    "-  determine the set $S$ of misclassified datapoints\n",
    "-  for every $\\mathbf{w}\\in S$ that is still misclassified, update the vector $\\mathbf{w}$\n",
    "-  only after that recalculate the set $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac792433-7ba8-483a-929f-c10fe0353b86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac792433-7ba8-483a-929f-c10fe0353b86",
    "outputId": "e0e5afe8-845a-4fb1-844e-f456798916a5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found separating hyperplane on step 215!\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Returns:\n",
    "        W: the final vector of weights for the separating hyperplane\n",
    "    \"\"\"\n",
    "\n",
    "for i in range(1000):\n",
    "    misclass = misclassified(X_train_flat_aug, y_train, W)\n",
    "    if len(misclass) == 0:\n",
    "        print(f\"Found separating hyperplane on step {i}!\")\n",
    "        break\n",
    "\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    ...\n",
    "    # ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f7ae1e-5c35-4105-8503-5aee57ea80f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_hyperplane(X, y, W):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        W: the final vector of weights for the separating hyperplane\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(2000):\n",
    "        misclass = misclassified(X, y, W)\n",
    "        if len(misclass) == 0:\n",
    "            print(f\"Found hyperplane on step {i}!\")\n",
    "            break\n",
    "    # ========= YOUR CODE STARTS HERE ========= #    \n",
    "        else:\n",
    "            for j in misclass:\n",
    "                datapoint = {'x': X[j], 'y': y[j]}\n",
    "                w_dot_x = np.dot(datapoint['x'], W)\n",
    "                \n",
    "                # Check if the point is still misclassified with the updated weights\n",
    "                if datapoint['y'] != np.sign(w_dot_x):\n",
    "                    W = W + (datapoint['x'] * datapoint['y'])        \n",
    "    return W\n",
    "    # ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "922a733c-1518-4549-a3b5-3f1ef9f3a037",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found hyperplane on step 862!\n"
     ]
    }
   ],
   "source": [
    "X_train_flat_aug = prep_data(X_train)\n",
    "W = initialize_weight_vector(X_train_flat_aug.shape[1])\n",
    "hyperplane = find_hyperplane(X_train_flat_aug, y_train.numpy(), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d7f57-bdec-4b35-9bca-22778c7778e9",
   "metadata": {
    "id": "151d7f57-bdec-4b35-9bca-22778c7778e9"
   },
   "source": [
    "### 2.7 Evaluate performance of the linear classifier on the test set **(0.3 pts)**\n",
    "\n",
    "Check your classifier on the test set. Think of possible metrics that characterize performance and comment on how good the classifier is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11a6c7-e4a3-439d-a836-19455f3d643a",
   "metadata": {
    "id": "ea11a6c7-e4a3-439d-a836-19455f3d643a"
   },
   "outputs": [],
   "source": [
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "    ...\n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xq-iaKD2jgYU",
   "metadata": {
    "id": "xq-iaKD2jgYU"
   },
   "source": [
    "## 3. Conclusions **(0.3 pts)**\n",
    "\n",
    "Summarize in a few sentences what you have learned and achieved by completing the tasks of this assignment\n",
    "\n",
    "\n",
    "\\### **YOUR ANSWER HERE** \\###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
